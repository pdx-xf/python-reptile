# BOSS直聘职位信息爬虫

这是一个用于爬取BOSS直聘职位信息的Python爬虫程序。该程序使用 Selenium 模拟浏览器行为，自动爬取 BOSS直聘 网站上的职位信息，支持自动化数据采集和存储。

## 功能特点

- 支持多关键词批量搜索
- 自动爬取职位列表详细信息
- 支持分页爬取（默认最多爬取10页）
- 智能反爬处理机制
- 随机延时和动态 User-Agent
- 多格式数据导出（JSON/CSV/Excel）
- 代理IP池支持
- 自动重试机制
- 完整的日志记录
- 定期数据自动保存

## 环境要求

- Python 3.7+
- Chrome 浏览器（最新版本）
- uv (Python包管理工具)
- 操作系统：Windows/Linux/MacOS
- 内存：至少 4GB RAM
- 磁盘空间：至少 1GB 可用空间

## 项目依赖

项目使用 pyproject.toml 管理依赖，主要依赖包：
- selenium >= 4.0.0
- webdriver-manager >= 3.8.0
- fake-useragent >= 1.1.0
- pandas >= 2.0.0
- openpyxl >= 3.1.0

## 快速开始

1. 安装 uv（如果尚未安装）：
```bash
pip install uv
```

2. 克隆项目到本地：
```bash
git clone [项目地址]
cd boss-spider
```

3. 创建并激活虚拟环境：
```bash
# 创建虚拟环境
uv venv

# Windows下激活虚拟环境
.venv\Scripts\activate

# Linux/MacOS下激活虚拟环境
source .venv/bin/activate
```

4. 安装项目依赖：
```bash
# 使用 uv 安装项目及其依赖
uv pip install .
```

## 配置说明

1. 首次运行前，复制配置模板：
```bash
cp config_example.py config.py
```

2. 编辑 `config.py` 文件，可配置以下内容：

- **搜索配置 (SEARCH_CONFIG)**
  - 关键词列表
  - 目标城市
  - 薪资范围
  - 经验要求
  - 学历要求
  - 公司规模等筛选条件

- **爬虫行为配置 (SPIDER_CONFIG)**
  - 最大爬取页数
  - 延时策略
  - 重试机制
  - 超时设置

- **数据存储配置 (STORAGE_CONFIG)**
  - JSON 文件路径
  - CSV 导出选项
  - Excel 导出选项

- **代理配置 (PROXY_CONFIG)**
  - 是否启用代理
  - 代理文件路径
  - 代理类型
  - 代理检查选项

- **浏览器配置 (BROWSER_CONFIG)**
  - 无头模式
  - User-Agent 轮换
  - 图片加载控制
  - 窗口大小

- **日志配置 (LOG_CONFIG)**
  - 日志级别
  - 日志文件路径
  - 日志格式

## 数据格式

爬取的职位信息将保存在 `jobs.json` 文件中，包含以下字段：

```json
{
    "职位": "Python开发工程师",
    "薪资": "15-30K",
    "公司": "示例科技有限公司",
    "地点": "深圳·南山区·科技园",
    "要求": "N/A",
    "公司类型": "互联网已上市1000-9999人",
    "页码": 1,
    "搜索关键词": "Python",
    "详细要求": "岗位职责：\n1. 负责项目后端开发...\n2. 参与系统架构设计...\n\n任职要求：\n1. 本科及以上学历...\n2. 三年以上开发经验..."
}
```

字段说明：
- `职位`：职位名称
- `薪资`：薪资范围，可能包含月薪制或年薪制说明
- `公司`：公司名称
- `地点`：工作地点，包含城市和具体区域
- `要求`：职位基本要求（大多数情况为 "N/A"）
- `公司类型`：公司行业、融资阶段和规模信息
- `页码`：数据来源页码
- `搜索关键词`：当前搜索使用的关键词
- `详细要求`：职位的详细描述，包含岗位职责和任职要求等

## 高级功能

- **多关键词搜索**：支持批量搜索多个职位关键词
- **自定义爬取规则**：通过配置文件自定义爬取行为
- **断点续爬**：定期保存数据，支持中断后继续
- **多格式导出**：支持导出为JSON/CSV/Excel格式
- **代理池支持**：可配置代理池避免IP限制
- **智能代理验证**：自动检查代理可用性
- **灵活配置**：所有功能都可通过配置文件调整

## 常见问题解决

1. **验证码处理**：
   - 程序会自动等待人工处理验证码
   - 验证通过后会自动继续爬取

2. **浏览器兼容**：
   - 确保Chrome浏览器版本与ChromeDriver版本匹配
   - 程序会自动下载匹配的ChromeDriver

3. **IP被封禁**：
   - 在配置文件中启用代理池
   - 调整爬取间隔（SPIDER_CONFIG中的delay设置）
   - 避免频繁运行

4. **数据保存问题**：
   - 程序每爬取10条数据自动保存一次
   - 支持多种格式备份
   - 可在配置文件中自定义保存路径

## 注意事项

- 首次运行前必须创建并配置 config.py
- 遵守网站的robots协议
- 建议设置合理的爬取间隔（默认3-5秒）
- 定期检查和更新Chrome浏览器版本
- 使用代理IP时注意代理质量
- 建议在闲时进行数据爬取

## 免责声明

本项目仅供学习和研究使用，请勿用于商业用途。使用本程序产生的任何后果由使用者自行承担。使用本程序即表示同意遵守相关法律法规和网站使用条款。 